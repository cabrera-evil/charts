# Default values for test.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx:stable-bookworm
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: "chart-example"

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

env: []

secret:
  enabled: false
  nameOverride: ""
    # The secret data to be stored
    # key1: value1
    # key2: value2
  data: {}

configMap:
  enabled: false
  nameOverride: ""
  data:
    # The config map data to be stored
    # key1: value1
    # key2: value2

podAnnotations: {}
podLabels: {}

podSecurityContext:
  fsGroup: 2000

securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 3000
  allowPrivilegeEscalation: false

service:
  type: ClusterIP
  port: 80
  annotations: {}
  # Optionally set a specific nodePort (only for NodePort/LoadBalancer)
  # nodePort: 30080

ingress:
  enabled: false
  className: traefik
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-production
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
    traefik.ingress.kubernetes.io/router.tls: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: Prefix
  tls:
    - hosts:
        - chart-example.local
      secretName: chart-example-cert

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

livenessProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 1
  successThreshold: 1
  failureThreshold: 3

readinessProbe:
  enabled: true
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 1
  successThreshold: 1
  failureThreshold: 3

startupProbe:
  enabled: false
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 1
  successThreshold: 1
  failureThreshold: 30

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Additional volumes on the output Deployment definition.
volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

# Additional volumeMounts on the output Deployment definition.
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

# Deployment strategy
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 25%
    maxUnavailable: 25%

# Pod Disruption Budget
podDisruptionBudget:
  enabled: false
  minAvailable: 1
  # maxUnavailable: 1

# Network Policy
networkPolicy:
  enabled: false
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector: {}
  egress:
    - to:
        - podSelector: {}

# Lifecycle hooks
lifecycle: {}
  # preStop:
  #   exec:
  #     command: ["/bin/sh", "-c", "sleep 15"]
  # postStart:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler"]

# Additional init containers
initContainers: []
# - name: init-myservice
#   image: busybox:1.28
#   command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']

# Additional sidecar containers
sidecars: []
# - name: sidecar
#   image: busybox:1.28
#   command: ['sh', '-c', 'tail -f /dev/null']

# Shared defaults applied to all jobs and cronjobs unless overridden per job
# These provide sensible defaults for common job configurations
jobsDefaults:
  annotations: {}
  labels: {}
  # Job spec defaults
  completions:  # Number of successful completions (default: 1)
  parallelism:  # Max pods running in parallel (default: 1)
  backoffLimit: 6  # Number of retries before marking job as failed
  ttlSecondsAfterFinished:  # Cleanup job after completion (e.g., 300 for 5 minutes)
  activeDeadlineSeconds:  # Max time for job to complete before termination
  completionMode:  # "NonIndexed" (default) or "Indexed" for indexed jobs
  backoffLimitPerIndex:  # Per-index backoff limit for indexed jobs (k8s 1.25+)
  maxFailedIndexes:  # Max failed indexes before job fails (k8s 1.25+)
  podReplacementPolicy:  # "TerminatingOrFailed" or "Failed" (k8s 1.28+)
  suspend: false  # Suspend job execution
  # Pod spec defaults
  restartPolicy: OnFailure  # "OnFailure" or "Never"
  serviceAccountName: ""
  priorityClassName: ""
  runtimeClassName: ""
  schedulerName: ""
  shareProcessNamespace: false
  dnsPolicy:
  dnsConfig: {}
  hostAliases: []
  terminationGracePeriodSeconds:
  topologySpreadConstraints: []
  # Container defaults
  image: {}  # Override with {repository, tag, pullPolicy}
  podAnnotations: {}
  podLabels: {}
  podSecurityContext: {}
  securityContext: {}
  resources: {}
  imagePullSecrets: []
  initContainers: []
  sidecars: []  # Sidecar containers for jobs
  env: []
  envFrom: []
  volumeMounts: []
  volumes: []
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # CronJob specific defaults (only used when job has a schedule)
  concurrencyPolicy: Allow  # "Allow", "Forbid", or "Replace"
  startingDeadlineSeconds:  # Deadline for starting if job misses scheduled time
  successfulJobsHistoryLimit: 3  # How many completed jobs to keep
  failedJobsHistoryLimit: 1  # How many failed jobs to keep
  timeZone:  # Timezone for schedule (e.g., "America/New_York", k8s 1.27+)

# Jobs and CronJobs - supports both one-time jobs and scheduled tasks
# Jobs can also be Helm hooks for lifecycle management (migrations, setup, etc.)
jobs: []
# Example 1: Database migration as a Helm hook (runs on install/upgrade)
# - name: db-migrate
#   enabled: true
#   # Helm hook configuration - runs before install/upgrade
#   hooks:
#     - pre-install
#     - pre-upgrade
#   hookWeight: "-5"  # Run before other pre-install/upgrade hooks
#   hookDeletePolicy:
#     - before-hook-creation  # Delete old job before creating new one
#   # Job configuration
#   completions: 1
#   backoffLimit: 3
#   ttlSecondsAfterFinished: 300  # Clean up after 5 minutes
#   activeDeadlineSeconds: 600  # Timeout after 10 minutes
#   restartPolicy: OnFailure
#   command: ["bundle", "exec", "rake", "db:migrate"]
#   image:
#     repository: myapp/backend
#     tag: "1.2.3"
#     pullPolicy: IfNotPresent
#   resources:
#     requests:
#       cpu: 100m
#       memory: 256Mi
#     limits:
#       cpu: 500m
#       memory: 512Mi

# Example 2: CronJob for scheduled cleanup task
# - name: cleanup
#   enabled: true
#   # CronJob schedule (daily at 2 AM)
#   schedule: "0 2 * * *"
#   timeZone: "UTC"  # Optional: specify timezone
#   concurrencyPolicy: Forbid  # Don't allow concurrent runs
#   successfulJobsHistoryLimit: 3
#   failedJobsHistoryLimit: 1
#   # Job spec
#   completions: 1
#   backoffLimit: 2
#   ttlSecondsAfterFinished: 86400  # Clean up after 24 hours
#   command: ["python", "cleanup.py"]
#   args: ["--days", "30"]
#   resources:
#     requests:
#       cpu: 50m
#       memory: 128Mi

# Example 3: Indexed job for parallel batch processing (k8s 1.24+)
# - name: batch-process
#   enabled: true
#   completionMode: Indexed
#   completions: 10  # Process 10 items
#   parallelism: 3  # Process 3 at a time
#   backoffLimitPerIndex: 2  # Allow 2 retries per index
#   maxFailedIndexes: 2  # Fail if more than 2 indexes fail
#   command: ["python", "process.py"]
#   args: ["--index", "$(JOB_COMPLETION_INDEX)"]
#   env:
#     - name: JOB_COMPLETION_INDEX
#       valueFrom:
#         fieldRef:
#           fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']

# Example 4: Job with pod failure policy (k8s 1.25+)
# - name: critical-task
#   enabled: true
#   backoffLimit: 6
#   # Advanced failure handling
#   podFailurePolicy:
#     rules:
#       # Ignore errors from specific exit codes
#       - action: Ignore
#         onExitCodes:
#           containerName: job
#           operator: In
#           values: [42, 43]  # Skip these exit codes
#       # Fail fast on OOM errors
#       - action: FailJob
#         onPodConditions:
#           - type: DisruptionTarget
#   command: ["./critical-task.sh"]

# Example 5: Post-install hook with success policy (k8s 1.30+)
# - name: init-data
#   enabled: true
#   hooks:
#     - post-install
#   hookDeletePolicy:
#     - hook-succeeded
#   completions: 3
#   parallelism: 3
#   # Succeed when 2 out of 3 pods succeed
#   successPolicy:
#     rules:
#       - succeededIndexes: "0,1"
#         succeededCount: 2
#   command: ["./init.sh"]

# Example 6: Simple one-time job with sidecars
# - name: backup
#   enabled: true
#   command: ["pg_dump", "-h", "postgres", "-U", "user", "dbname"]
#   sidecars:
#     - name: log-shipper
#       image: fluent/fluent-bit:latest
#       volumeMounts:
#         - name: logs
#           mountPath: /logs
#   volumes:
#     - name: logs
#       emptyDir: {}
#   volumeMounts:
#     - name: logs
#       mountPath: /var/log
